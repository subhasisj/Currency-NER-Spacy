{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600259168020",
   "display_name": "Python 3.8.5 64-bit ('refinitiv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model Evaluation on Test Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from src.dataloader import DataLoader\n",
    "from src.logger import Logger\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from textblob import TextBlob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laod testing labels\n",
    "df_test_labels = pd.read_csv(os.path.join('..','Data','test_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       doc_id              phrase  ric\n0  0563_20171129_nL3N1NZ1MX_1              Aussie  AUD\n1  0563_20171129_nL3N1NZ1MX_1  New Zealand dollar  NZD\n2  0563_20171129_nL3N1NZ1MX_1   Australian dollar  AUD\n3  0563_20171129_nL3N1NZ1MX_1         kiwi dollar  NZD\n4  0563_20171129_nL3N1NZ1MX_1   Australian dollar  AUD\n5  0563_20171129_nL3N1NZ1MX_1         U.S. dollar  USD\n6  0730_20180329_nFCT29YLPY_1                 USD  USD\n7  0730_20180329_nFCT29YLPY_1                 yen  JPY\n8  0730_20180329_nFCT29YLPY_1                 USD  USD\n9  0730_20180329_nFCT29YLPY_1                 JPY  JPY",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>phrase</th>\n      <th>ric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>Aussie</td>\n      <td>AUD</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>New Zealand dollar</td>\n      <td>NZD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>Australian dollar</td>\n      <td>AUD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>kiwi dollar</td>\n      <td>NZD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>Australian dollar</td>\n      <td>AUD</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0563_20171129_nL3N1NZ1MX_1</td>\n      <td>U.S. dollar</td>\n      <td>USD</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0730_20180329_nFCT29YLPY_1</td>\n      <td>USD</td>\n      <td>USD</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0730_20180329_nFCT29YLPY_1</td>\n      <td>yen</td>\n      <td>JPY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0730_20180329_nFCT29YLPY_1</td>\n      <td>USD</td>\n      <td>USD</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0730_20180329_nFCT29YLPY_1</td>\n      <td>JPY</td>\n      <td>JPY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_test_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets group by the file names and collect the unique entities\n",
    "df_collected_entities = df_test_labels.groupby('doc_id')['ric'].agg(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       doc_id  \\\n0  0002_20171221_nFCT20VZKW_1   \n1                   001_yahoo   \n2  0243_20170120_nDJMS0289D_1   \n3  0244_20180416_nFCT16ZCHB_1   \n4  0245_20180416_nNRA5ww5ev_1   \n\n                                                 ric  \n0      {NZD, SEK, AUD, GBP, NOK, JPY, EUR, USD, CAD}  \n1  {TWD, PHP, SGD, THB, MYR, IDR, EUR, INR, CNY, ...  \n2                               {PLN, HUF, EUR, CZK}  \n3           {NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}  \n4  {TWD, PHP, MYR, THB, KRW, IDR, JPY, INR, CNY, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>ric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0002_20171221_nFCT20VZKW_1</td>\n      <td>{NZD, SEK, AUD, GBP, NOK, JPY, EUR, USD, CAD}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001_yahoo</td>\n      <td>{TWD, PHP, SGD, THB, MYR, IDR, EUR, INR, CNY, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0243_20170120_nDJMS0289D_1</td>\n      <td>{PLN, HUF, EUR, CZK}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0244_20180416_nFCT16ZCHB_1</td>\n      <td>{NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0245_20180416_nNRA5ww5ev_1</td>\n      <td>{TWD, PHP, MYR, THB, KRW, IDR, JPY, INR, CNY, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_collected_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 70 entries, 0 to 69\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   doc_id  70 non-null     object\n 1   ric     70 non-null     object\ndtypes: object(2)\nmemory usage: 1.2+ KB\n"
    }
   ],
   "source": [
    "df_collected_entities.info()"
   ]
  },
  {
   "source": [
    "__We have to extract entities for 70 documents__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = spacy.load(os.path.join('..','model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load files one \n",
    "test_files_path = os.path.join('..','Data','test')\n",
    "time_created = datetime.now()\n",
    "logger = Logger(f'Evaluation_logs_{time_created.date()}_{time_created.strftime(\"%H%M%S\")}.log')\n",
    "data_loader = DataLoader(logger,test_files_path)\n",
    "text_preprocessor = TextPreprocessor(logger)\n",
    "\n",
    "files = []\n",
    "entities = []\n",
    "for index, row in df_collected_entities.iterrows():\n",
    "    text = data_loader.read_file(row['doc_id'])\n",
    "    processed_text = TextBlob(text)\n",
    "\n",
    "     # Check sentence wise sentiment and extract entities\n",
    "\n",
    "    entities_found = []\n",
    "    for s in processed_text.sentences:\n",
    "        if s.sentiment.polarity > 0.05 or s.sentiment.polarity < -0.05:\n",
    "            text = text_preprocessor.clean_text(s)\n",
    "            doc = model(text)\n",
    "            for ent in doc.ents:\n",
    "                entities_found.append(ent.label_)\n",
    "\n",
    "    if len(entities_found)>0:\n",
    "        files.append(row['doc_id'])\n",
    "        entities.append(set(entities_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_entity = {'file':files,'detected_entities':entities}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_entity = pd.DataFrame(file_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                         file  \\\n0  0002_20171221_nFCT20VZKW_1   \n1                   001_yahoo   \n2  0243_20170120_nDJMS0289D_1   \n3  0244_20180416_nFCT16ZCHB_1   \n4  0245_20180416_nNRA5ww5ev_1   \n\n                                   detected_entities  \n0  {NZD, SEK, AUD, GBP, SGD, NOK, JPY, EUR, USD, ...  \n1                                    {CNY, USD, IDR}  \n2                               {PLN, EUR, USD, NOK}  \n3           {NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}  \n4                          {TWD, IDR, RUB, INR, USD}  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>detected_entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0002_20171221_nFCT20VZKW_1</td>\n      <td>{NZD, SEK, AUD, GBP, SGD, NOK, JPY, EUR, USD, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001_yahoo</td>\n      <td>{CNY, USD, IDR}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0243_20170120_nDJMS0289D_1</td>\n      <td>{PLN, EUR, USD, NOK}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0244_20180416_nFCT16ZCHB_1</td>\n      <td>{NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0245_20180416_nNRA5ww5ev_1</td>\n      <td>{TWD, IDR, RUB, INR, USD}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "df_file_entity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_pred = df_file_entity.merge(df_collected_entities, left_on = 'file',right_on='doc_id',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                         file  \\\n0  0002_20171221_nFCT20VZKW_1   \n1                   001_yahoo   \n2  0243_20170120_nDJMS0289D_1   \n3  0244_20180416_nFCT16ZCHB_1   \n4  0245_20180416_nNRA5ww5ev_1   \n\n                                   detected_entities  \\\n0  {NZD, SEK, AUD, GBP, SGD, NOK, JPY, EUR, USD, ...   \n1                                    {CNY, USD, IDR}   \n2                               {PLN, EUR, USD, NOK}   \n3           {NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}   \n4                          {TWD, IDR, RUB, INR, USD}   \n\n                       doc_id  \\\n0  0002_20171221_nFCT20VZKW_1   \n1                   001_yahoo   \n2  0243_20170120_nDJMS0289D_1   \n3  0244_20180416_nFCT16ZCHB_1   \n4  0245_20180416_nNRA5ww5ev_1   \n\n                                                 ric  \n0      {NZD, SEK, AUD, GBP, NOK, JPY, EUR, USD, CAD}  \n1  {TWD, PHP, SGD, THB, MYR, IDR, EUR, INR, CNY, ...  \n2                               {PLN, HUF, EUR, CZK}  \n3           {NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}  \n4  {TWD, PHP, MYR, THB, KRW, IDR, JPY, INR, CNY, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>detected_entities</th>\n      <th>doc_id</th>\n      <th>ric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0002_20171221_nFCT20VZKW_1</td>\n      <td>{NZD, SEK, AUD, GBP, SGD, NOK, JPY, EUR, USD, ...</td>\n      <td>0002_20171221_nFCT20VZKW_1</td>\n      <td>{NZD, SEK, AUD, GBP, NOK, JPY, EUR, USD, CAD}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001_yahoo</td>\n      <td>{CNY, USD, IDR}</td>\n      <td>001_yahoo</td>\n      <td>{TWD, PHP, SGD, THB, MYR, IDR, EUR, INR, CNY, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0243_20170120_nDJMS0289D_1</td>\n      <td>{PLN, EUR, USD, NOK}</td>\n      <td>0243_20170120_nDJMS0289D_1</td>\n      <td>{PLN, HUF, EUR, CZK}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0244_20180416_nFCT16ZCHB_1</td>\n      <td>{NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}</td>\n      <td>0244_20180416_nFCT16ZCHB_1</td>\n      <td>{NZD, AUD, GBP, JPY, EUR, CHF, USD, CAD}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0245_20180416_nNRA5ww5ev_1</td>\n      <td>{TWD, IDR, RUB, INR, USD}</td>\n      <td>0245_20180416_nNRA5ww5ev_1</td>\n      <td>{TWD, PHP, MYR, THB, KRW, IDR, JPY, INR, CNY, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df_true_pred.head()"
   ]
  },
  {
   "source": [
    "_It can be concluded that we can improve the performace even further by using a dataset which has __Inside–outside–beginning (tagging)__ _\n",
    "\n",
    "1.  __I have currently trained the SPACY NER model for 10 iterations, it could have been trained further__\n",
    "2.  __I would have used a Bi-LSTM or BERT NER for undertanding the context better.__\n",
    "\n",
    "3.  __While I have tried to stick to proper coding convention for the major part, but it can be better given more time. I might have used dockers for easy reproducibility__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Assumptions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I have taken some assumptions for creating the training datasets, Cleaning and during inferencing.\n",
    "\n",
    "1. For creating the training data, I have used Spacy's Matcher object for matching tokens from the lexicon phrases and tagging them to an ID which would be used to train the entity label. I studied the pattern for few of the documents in the `Exploration.ipynb` file. And while matching I considered the POS tags as well. This can ofcourse be better structured, may be using PhraseMatcher or by adding more rules for the matching like an additional Dependency Tag.\n",
    "\n",
    "2. During Cleaning, I performed some basic tasks like removing URL's, spaces , text between brackets. While this topic can alone span for days and there is literally no restriction on the amount of cleaning activities, I decided to continue with the basic stuff. I did not use a lower case conversion because in most cases the entities are CASE-SENSITIVE.\n",
    "\n",
    "3. While Inferencing, I chose to convert the document into granular sentences, then calculate the Sentiment (TEXTBLOB Polarity) and only when there is a hint of positive or negative Sentiment , I proceed and extract the Entities from the document.\n",
    "\n",
    "4. For most part the code (Variables, Classes, logs ,functions etc) are self-explanatory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}